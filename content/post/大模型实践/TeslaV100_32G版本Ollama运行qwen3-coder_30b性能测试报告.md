---
title: 'TeslaV100_32G版本Ollama运行qwen3-coder_30b性能测试报告'
categories: ["大模型实践"]
date: 2026-02-13T01:22:05+08:00
lastmod: 2026-02-13T01:22:05+08:00
encrypted: false
---
# Tesla V100（32G版本）Ollama 运行qwen3-coder:30b性能测试报告

## 测试概要

**模型**: `zdolny/qwen3-coder58k-tools:latest`（尺寸为30b）
**上下文窗口**: 58,000 tokens
**测试环境**: Windows + Tesla V100-SXM2-32GB GPU
**测试日期**: 2025-12-13

本报告详细测试了模型在不同上下文长度和并发场景下的性能表现，重点关注首个 Token 时间和 Token 生成速率两个关键指标。

## 系统环境信息


| 项目             | 配置                               |
| ---------------- | ---------------------------------- |
| **GPU**          | Tesla V100-SXM2-32GB               |
| **GPU 显存使用** | 22,910MiB / 32,768MiB (70% 利用率) |
| **驱动版本**     | 572.83                             |
| **CUDA 版本**    | 12.8                               |
| **GPU 温度**     | 35°C                              |
| **GPU TDP**      | 300W (最大功耗)                    |

## 详细性能测试结果

### 1. 不同上下文长度性能对比


| 测试场景     | 上下文长度    | 首个 Token 时间 | Token 生成速率     | 生成 Token 数量 |
| ------------ | ------------- | --------------- | ------------------ | --------------- |
| **短提示**   | ~9 tokens     | **82ms**        | **84.7 tokens/秒** | 10 tokens       |
| **中等提示** | ~13 tokens    | **291ms**       | **75.3 tokens/秒** | 270 tokens      |
| **高上下文** | 3,015 tokens  | **5.42 秒**     | **75.3 tokens/秒** | 64 tokens       |
| **大上下文** | 15,018 tokens | **17.43 秒**    | **71.8 tokens/秒** | 19 tokens       |

### 2. 并发性能测试


| 指标                    | 结果               |
| ----------------------- | ------------------ |
| **并发请求数**          | 3                  |
| **成功率**              | 3/3 (100%)         |
| **总测试耗时**          | 16.56 秒           |
| **平均首个 Token 时间** | **0.18 秒**        |
| **平均 Token 生成速率** | **75.6 tokens/秒** |
| **总生成 Token 数量**   | 1,060 tokens       |

### 3. 最大上下文长度性能推测

基于实测数据的线性外推：


| 上下文长度        | 预计首个 Token 时间 | 预计提示处理时间 |
| ----------------- | ------------------- | ---------------- |
| 3,015 tokens      | 5.42 秒             | 2.10 秒          |
| 15,018 tokens     | 17.43 秒            | 14.14 秒         |
| **58,000 tokens** | **~67 秒**          | **~54 秒**       |

## 关键性能洞察

### 🚀 首个 Token 延迟分析

- **极短提示**: ~82ms（接近实时响应）
- **中等提示**: ~291ms（用户可接受范围）
- **高上下文提示 (3K tokens)**: ~5.42 秒
- **大上下文提示 (15K tokens)**: ~17.43 秒
- **最大上下文推测 (58K tokens)**: **~67 秒**

> **结论**: 首个 Token 时间与上下文长度近似线性增长，这是 Transformer 架构的正常特性。

### ⚡ Token 生成吞吐量

- **性能稳定性**: 71-85 tokens/秒
- **上下文长度影响**: 几乎无影响，生成速率保持稳定
- **并发场景表现**: 75.6 tokens/秒（与单请求基本一致）
- **整体评价**: 对于 58K 上下文窗口的大模型，这是优秀的性能表现

### 🔧 模型加载性能

- **短提示加载**: ~44ms（GPU 内存管理高效）
- **高上下文加载**: ~3.3 秒（稳定表现）
- **内存管理**: 模型在内存中保持加载状态，便于后续请求

### 📈 上下文长度影响

- **提示处理时间**: 与上下文长度近似线性增长
- **3,015 tokens**: 2.10 秒处理时间
- **15,018 tokens**: 14.14 秒处理时间
- **58,000 tokens 推测**: ~54 秒处理时间

### 🔄 并发性能表现

- **并发处理能力**: Ollama 能够有效处理多请求并发
- **资源利用率**: 3个并发请求全部成功完成
- **性能保持**: 平均首个 Token 时间仅 0.18 秒，Token 生成速率稳定

### 💻 GPU 利用效率

- **Tesla V100 GPU**: 使用效率高
- **显存利用率**: 70% 表明资源分配良好
- **散热性能**: 35°C 低温运行，散热优秀

## 性能建议

### 1. 应用场景选择

- **实时交互应用**: 适合短到中等上下文（< 1K tokens），首个 Token 时间 < 300ms
- **长文档处理**: 可处理大上下文，但需考虑 ~17 秒的首个 Token 延迟
- **批量处理**: 并发性能优秀，适合多任务并行处理

### 2. 性能优化建议

- **上下文管理**: 合理控制上下文长度，避免不必要的长上下文
- **缓存策略**: 利用模型在内存中的持久化特性，减少重复加载
- **并发调度**: 可安全使用并发请求，Ollama 具备良好的并发处理能力

### 3. 硬件配置建议

- **GPU 显存**: 32GB 显存可轻松处理 58K 上下文模型
- **散热要求**: 即使高负载下温度也保持在 35°C，散热需求适中
- **电源需求**: 300W 电源足够支持 Tesla V100 运行

## 测试方法说明

### 数据采集方式

- **API 端点**: `POST /api/generate` (stream=false)
- **性能指标**: 直接从 Ollama API 响应中提取内置性能数据
- **关键指标计算**:
  - 首个 Token 时间 = `load_duration` + `prompt_eval_duration`
  - Token 生成速率 = `eval_count` / `eval_duration`

### 测试脚本

- **单请求测试**: Python requests 库直接调用 API
- **并发测试**: ThreadPoolExecutor 实现 3 线程并发
- **大上下文生成**: 重复句子构造指定长度上下文

### 测试限制

- **最大上下文测试**: 由于时间限制，未直接测试 58K tokens，采用线性外推
- **硬件环境**: 单 GPU 测试，未测试多 GPU 扩展性
- **网络环境**: 本地回环测试，未考虑网络延迟影响

## 结论

`zdolny/qwen3-coder58k-tools:latest` 模型在 Tesla V100 GPU 上表现出色：

GLM-4.7-Flash: ███████████████████████ 31.02 tokens/s (基准)
Qwen3-Coder:     ████████░░░░░░░░░░░░░ 10.31 tokens/s (33% of GLM)
Minimax M2.1:     ███████░░░░░░░░░░░░░░  9.30 tokens/s (30% of GLM)

✅ **优势**:

- 稳定的 Token 生成速率 (71-85 tokens/秒)
- 优秀的并发处理能力
- 高效的 GPU 显存利用
- 良好的散热和功耗表现

⚠️ **注意事项**:

- 大上下文场景下首个 Token 延迟显著增加
- 58K tokens 上下文预计需要 ~67 秒首个 Token 时间
- 建议根据应用场景合理选择上下文长度

该模型非常适合需要大上下文窗口的应用场景，在保证长上下文能力的同时，维持了优秀的 Token 生成性能和并发处理能力。
