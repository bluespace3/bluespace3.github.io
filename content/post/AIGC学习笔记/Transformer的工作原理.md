Transformer的工作原理

拿到句子：从前有个国王，他有个女儿

1.Token化：分词为得到token:从前|有个|国王|，|他|有个|女儿

2.词嵌入、词向量：将每个token进行向量化（该token和其他对象的关联关系打分）->转换成一堆数字,例如国王[0.9（权力）,0.8（城堡），0.7（严肃）....]，女儿[....]

3.注意力机制：

​	编码器处理：分析问题用注意力划重点（如国王和女儿关系等）

​	解码器处理：边编故事边查表，用注意力确保连贯，生成内容。

整体：进入计算->注意力机制->计算token与上下文的关系 ，据概率得到新词

4.不断生成：再将新词进行上述计算，得到下一个新词。像玩文字无限接龙，每次参考所有历史信息，用注意力决定下一步重点。